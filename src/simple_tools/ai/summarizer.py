"""æ–‡æ¡£è‡ªåŠ¨æ‘˜è¦æ¨¡å—

ä½¿ç”¨DeepSeek APIå¯¹å„ç§æ ¼å¼çš„æ–‡æ¡£ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ã€‚
æ”¯æŒtxtã€mdã€pdfã€docxç­‰æ ¼å¼ã€‚
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

import logfire
from docx import Document
from pydantic import BaseModel, Field
from pypdf import PdfReader

from ..utils.errors import ToolError
from .deepseek_client import DeepSeekClient
from .prompts import PromptManager


class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯æ¨¡å‹"""

    path: Path = Field(..., description="æ–‡æ¡£è·¯å¾„")
    title: str = Field(..., description="æ–‡æ¡£æ ‡é¢˜")
    doc_type: str = Field(..., description="æ–‡æ¡£ç±»å‹")
    word_count: int = Field(..., description="å­—æ•°ç»Ÿè®¡")
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    metadata: dict[str, Any] = Field(default_factory=dict, description="å…ƒæ•°æ®")


class SummaryResult(BaseModel):
    """æ‘˜è¦ç»“æœæ¨¡å‹"""

    file_path: Path = Field(..., description="æ–‡ä»¶è·¯å¾„")
    summary: str = Field(..., description="æ‘˜è¦å†…å®¹")
    word_count: int = Field(..., description="åŸæ–‡å­—æ•°")
    summary_length: int = Field(..., description="æ‘˜è¦å­—æ•°")
    doc_type: str = Field(..., description="æ–‡æ¡£ç±»å‹")
    cached: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨ç¼“å­˜")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")


class BatchSummaryResult(BaseModel):
    """æ‰¹é‡æ‘˜è¦ç»“æœ"""

    total: int = Field(0, description="æ€»æ–‡ä»¶æ•°")
    success: int = Field(0, description="æˆåŠŸæ•°")
    failed: int = Field(0, description="å¤±è´¥æ•°")
    results: list[SummaryResult] = Field(
        default_factory=list, description="æ‘˜è¦ç»“æœåˆ—è¡¨"
    )


class DocumentSummarizer:
    """æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨"""

    SUPPORTED_FORMATS = {
        ".txt": "text",
        ".md": "markdown",
        ".rst": "restructuredtext",
        ".pdf": "pdf",
        ".docx": "word",
        ".doc": "word",
    }

    MAX_CONTENT_LENGTH = 10000

    def __init__(self, client: Optional[DeepSeekClient] = None):
        """åˆå§‹åŒ–æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨

        Args:
            client: å¯é€‰çš„DeepSeekå®¢æˆ·ç«¯å®ä¾‹

        """
        self.client = client or DeepSeekClient()
        self._summary_cache: dict[str, str] = {}
        logfire.info("åˆå§‹åŒ–æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨")

    def extract_document_content(self, file_path: Path) -> DocumentInfo:
        """æå–æ–‡æ¡£å†…å®¹

        Args:
            file_path: éœ€è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„

        Returns:
            DocumentInfo: åŒ…å«æ–‡æ¡£ä¿¡æ¯çš„å¯¹è±¡

        Raises:
            ToolError: å½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼ä¸æ”¯æŒæ—¶æŠ›å‡º

        """
        if not file_path.exists():
            raise ToolError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", "FILE_NOT_FOUND")

        extension = file_path.suffix.lower()
        if extension not in self.SUPPORTED_FORMATS:
            raise ToolError(
                f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {extension}",
                "UNSUPPORTED_FORMAT",
            )

        doc_type = self.SUPPORTED_FORMATS[extension]

        if doc_type in ["text", "markdown", "restructuredtext"]:
            content = self._extract_text_content(file_path)
        elif doc_type == "pdf":
            content = self._extract_pdf_content(file_path)
        elif doc_type == "word":
            content = self._extract_docx_content(file_path)
        else:
            raise ToolError(f"æœªå®ç°çš„æ–‡æ¡£ç±»å‹å¤„ç†: {doc_type}", "NOT_IMPLEMENTED")

        if len(content) > self.MAX_CONTENT_LENGTH:
            content = content[: self.MAX_CONTENT_LENGTH] + "\n...(å†…å®¹å·²æˆªæ–­)"

        word_count = self._count_words(content)

        return DocumentInfo(
            path=file_path,
            title=file_path.stem,
            doc_type=doc_type,
            word_count=word_count,
            content=content,
        )

    def _extract_text_content(self, file_path: Path) -> str:
        """æå–æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, encoding="utf-8") as f:
                return f.read()
        except UnicodeDecodeError:
            with open(file_path, encoding="gbk", errors="ignore") as f:
                return f.read()

    def _extract_pdf_content(self, file_path: Path) -> str:
        """æå–PDFæ–‡ä»¶å†…å®¹"""
        try:
            reader = PdfReader(file_path)
            content = []
            for page in reader.pages:
                text = page.extract_text()
                if text:
                    content.append(text)
            return "\n".join(content)
        except Exception as e:
            logfire.error(f"PDFå†…å®¹æå–å¤±è´¥: {file_path} - {e}")
            raise ToolError(f"æ— æ³•è¯»å–PDFæ–‡ä»¶: {e}", "PDF_READ_ERROR")

    def _extract_docx_content(self, file_path: Path) -> str:
        """æå–Wordæ–‡æ¡£å†…å®¹"""
        try:
            doc = Document(str(file_path))
            content = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    content.append(paragraph.text)
            return "\n".join(content)
        except Exception as e:
            logfire.error(f"Wordæ–‡æ¡£å†…å®¹æå–å¤±è´¥: {file_path} - {e}")
            raise ToolError(f"æ— æ³•è¯»å–Wordæ–‡æ¡£: {e}", "DOCX_READ_ERROR")

    def _count_words(self, text: str) -> int:
        """æ··åˆä¸­è‹±æ–‡å­—æ•°ç»Ÿè®¡"""
        chinese_chars = 0
        non_chinese_text = []

        for char in text:
            if "\u4e00" <= char <= "\u9fff":
                chinese_chars += 1
                non_chinese_text.append(" ")
            else:
                non_chinese_text.append(char)

        english_text = "".join(non_chinese_text)
        english_text = "".join(
            [c if c.isalnum() or c.isspace() else " " for c in english_text]
        )
        english_words = len([word for word in english_text.split() if word])

        return chinese_chars + english_words

    async def summarize_document(
        self,
        file_path: Path,
        target_length: int = 200,
        language: str = "zh",
        use_cache: bool = True,
    ) -> SummaryResult:
        """ç”Ÿæˆå•ä¸ªæ–‡æ¡£æ‘˜è¦"""
        try:
            doc_info = self.extract_document_content(file_path)

            cache_key = f"{file_path}:{target_length}:{language}"
            if use_cache and cache_key in self._summary_cache:
                logfire.info(f"ä½¿ç”¨ç¼“å­˜æ‘˜è¦: {file_path}")
                return SummaryResult(
                    file_path=file_path,
                    summary=self._summary_cache[cache_key],
                    word_count=doc_info.word_count,
                    summary_length=len(self._summary_cache[cache_key]),
                    doc_type=doc_info.doc_type,
                    cached=True,
                )

            prompt = PromptManager.format(
                "document_summarize",
                title=doc_info.title,
                doc_type=doc_info.doc_type,
                word_count=doc_info.word_count,
                content=doc_info.content,
                length=target_length,
            )

            with logfire.span(
                "summarize_document",
                attributes={
                    "file": str(file_path),
                    "doc_type": doc_info.doc_type,
                    "word_count": doc_info.word_count,
                },
            ):
                summary = await self.client.simple_chat(prompt)
                summary = summary.strip().strip('"').strip("'")

                if use_cache:
                    self._summary_cache[cache_key] = summary

                return SummaryResult(
                    file_path=file_path,
                    summary=summary,
                    word_count=doc_info.word_count,
                    summary_length=len(summary),
                    doc_type=doc_info.doc_type,
                    cached=False,
                )

        except Exception as e:
            logfire.error(f"æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå¤±è´¥: {file_path} - {e}")
            return SummaryResult(
                file_path=file_path,
                summary="",
                word_count=0,
                summary_length=0,
                doc_type="unknown",
                error=str(e),
            )

    async def summarize_batch(
        self,
        file_paths: list[Path],
        target_length: int = 200,
        language: str = "zh",
        max_concurrent: int = 3,
        use_cache: bool = True,
    ) -> BatchSummaryResult:
        """æ‰¹é‡ç”Ÿæˆæ–‡æ¡£æ‘˜è¦

        Args:
            file_paths: éœ€è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            target_length: ç›®æ ‡æ‘˜è¦é•¿åº¦
            language: ç”Ÿæˆè¯­è¨€
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            BatchSummaryResult: æ‰¹é‡å¤„ç†ç»“æœ

        """
        result = BatchSummaryResult(total=len(file_paths))
        semaphore = asyncio.Semaphore(max_concurrent)

        async def summarize_with_limit(file_path: Path) -> SummaryResult:
            async with semaphore:
                return await self.summarize_document(
                    file_path, target_length, language, use_cache
                )

        with logfire.span(
            "batch_summarize",
            attributes={
                "file_count": len(file_paths),
                "max_concurrent": max_concurrent,
            },
        ):
            tasks = [summarize_with_limit(fp) for fp in file_paths]
            summaries = await asyncio.gather(*tasks)

            for summary in summaries:
                result.results.append(summary)
                if summary.error:
                    result.failed += 1
                else:
                    result.success += 1

            logfire.info(f"æ‰¹é‡æ‘˜è¦å®Œæˆ: æˆåŠŸ={result.success}, å¤±è´¥={result.failed}")

        return result

    def save_summaries(
        self, results: list[SummaryResult], output_path: Path, format: str = "json"
    ) -> None:
        """ä¿å­˜æ‘˜è¦ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: æ‘˜è¦ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: è¾“å‡ºæ ¼å¼ (json/markdown)

        """
        if format == "json":
            self._save_as_json(results, output_path)
        elif format == "markdown":
            self._save_as_markdown(results, output_path)
        else:
            raise ToolError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {format}", "INVALID_FORMAT")

    def _save_as_json(self, results: list[SummaryResult], output_path: Path) -> None:
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        data = []
        for result in results:
            if not result.error:
                data.append(
                    {
                        "file": str(result.file_path),
                        "summary": result.summary,
                        "word_count": result.word_count,
                        "doc_type": result.doc_type,
                    }
                )

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _save_as_markdown(
        self, results: list[SummaryResult], output_path: Path
    ) -> None:
        """ä¿å­˜ä¸ºMarkdownæ ¼å¼

        Args:
            results: æ‘˜è¦ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        """
        content = ["# æ–‡æ¡£æ‘˜è¦æ±‡æ€»\n"]
        content.append(f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        for result in results:
            if not result.error:
                content.append(f"## ğŸ“„ {result.file_path.name}\n")
                content.append(f"- æ–‡æ¡£ç±»å‹ï¼š{result.doc_type}")
                content.append(f"- åŸæ–‡å­—æ•°ï¼š{result.word_count}")
                content.append(f"- æ‘˜è¦å­—æ•°ï¼š{result.summary_length}\n")
                content.append(f"**æ‘˜è¦ï¼š**\n{result.summary}\n")
                content.append("---\n")

        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(content))
