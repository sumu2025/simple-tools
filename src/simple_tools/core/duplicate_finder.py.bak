"""重复文件检测模块."""

import asyncio
import hashlib
import os
from collections import defaultdict
from pathlib import Path
from typing import Any, Optional

import click
import logfire
from pydantic import BaseModel, Field

from simple_tools._typing import argument, command, option, pass_context
from simple_tools.utils.errors import ErrorContext, ToolError

from ..ai.config import AIConfig
from ..ai.version_analyzer import VersionAnalyzer
from ..utils.formatter import DuplicateData, format_output
from ..utils.progress import process_with_progress


class DuplicateConfig(BaseModel):
    """重复文件检测配置."""

    path: str = Field(..., description="扫描路径")
    recursive: bool = Field(True, description="是否递归扫描子目录")
    min_size: int = Field(1, description="最小文件大小（字节）")
    extensions: Optional[list[str]] = Field(None, description="指定文件扩展名")


class FileInfo(BaseModel):
    """文件信息."""

    path: Path = Field(..., description="文件路径")
    size: int = Field(..., description="文件大小（字节）")
    hash: Optional[str] = Field(None, description="文件MD5哈希值")


class DuplicateGroup(BaseModel):
    """重复文件组."""

    hash: str = Field(..., description="文件MD5哈希值")
    size: int = Field(..., description="文件大小（字节）")
    count: int = Field(..., description="重复文件数量")
    files: list[Path] = Field(..., description="重复文件路径列表")
    potential_save: int = Field(..., description="可节省空间（字节）保留一个文件")


class DuplicateFinder:
    """重复文件检测器."""

    def __init__(self, config: DuplicateConfig):
        """初始化重复文件检测器.

        参数：config - 检测配置对象
        """
        self.config = config
        logfire.info("初始化重复文件检测器", attributes={"config": config.model_dump()})

    def _calculate_file_hash(self, file_path: Path) -> str:
        """计算文件的MD5哈希值（分块读取，避免大文件内存问题）.

        参数：file_path - 文件路径对象
        返回：MD5哈希值字符串
        """
        hash_md5 = hashlib.md5()

        try:
            with open(file_path, "rb") as f:
                # 每次读取8KB，避免大文件占用过多内存
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except PermissionError:
            context = ErrorContext(
                operation="计算文件哈希",
                file_path=str(file_path),
                details={"error_type": "权限不足"},
            )
            raise ToolError(
                f"无法读取文件 {file_path}",
                error_code="PERMISSION_DENIED",
                context=context,
                suggestions=[
                    "检查文件权限设置",
                    "尝试使用管理员权限运行",
                    "确认文件未被其他程序占用",
                ],
            )
        except FileNotFoundError:
            context = ErrorContext(
                operation="计算文件哈希",
                file_path=str(file_path),
                details={"error_type": "文件不存在"},
            )
            raise ToolError(
                f"文件不存在 {file_path}",
                error_code="FILE_NOT_FOUND",
                context=context,
                suggestions=["重新运行扫描", "检查文件是否被移动或删除"],
            )
        except OSError as e:
            context = ErrorContext(
                operation="计算文件哈希",
                file_path=str(file_path),
                details={"error_type": "系统错误", "original_error": str(e)},
            )
            raise ToolError(
                f"读取文件失败 {file_path}",
                error_code="OPERATION_FAILED",
                context=context,
                original_error=e,
                suggestions=[
                    "检查磁盘空间是否充足",
                    "检查文件系统是否正常",
                    "尝试重新运行命令",
                ],
            )
        except Exception as e:
            logfire.error(
                f"计算文件哈希失败: {file_path}", attributes={"error": str(e)}
            )
            context = ErrorContext(
                operation="计算文件哈希",
                file_path=str(file_path),
                details={"error_type": "未知错误", "original_error": str(e)},
            )
            raise ToolError(
                f"计算文件哈希失败 {file_path}",
                error_code="GENERAL_ERROR",
                context=context,
                original_error=e,
                suggestions=[
                    "检查文件是否损坏",
                    "尝试重新运行命令",
                    "如果问题持续，请报告此错误",
                ],
            )

    def _should_include_file(self, file_path: Path) -> bool:
        """判断文件是否应该包含在检测范围内.

        参数：file_path - 文件路径对象
        返回：布尔值，True表示应该包含
        """
        # 检查文件大小是否满足最小要求
        try:
            file_size = file_path.stat().st_size
            if file_size < self.config.min_size:
                return False
        except OSError:
            return False

        # 检查文件扩展名是否在指定列表中
        if self.config.extensions:
            file_ext = file_path.suffix.lower()
            return file_ext in [ext.lower() for ext in self.config.extensions]

        return True

    def _scan_files(self) -> list[FileInfo]:
        """扫描目录获取所有符合条件的文件信息.

        返回：FileInfo对象列表
        """
        with logfire.span("scan_files", attributes={"path": self.config.path}):
            files = []
            scan_path = Path(self.config.path)

            # 检查路径是否存在
            if not scan_path.exists():
                context = ErrorContext(
                    operation="扫描目录",
                    file_path=str(scan_path),
                    details={"error_type": "路径不存在"},
                )
                raise ToolError(
                    f"扫描路径不存在: {self.config.path}",
                    error_code="FILE_NOT_FOUND",
                    context=context,
                    suggestions=[
                        "检查路径拼写是否正确",
                        "确认目录是否存在",
                        "使用绝对路径重试",
                    ],
                )

            # 检查是否为目录
            if not scan_path.is_dir():
                context = ErrorContext(
                    operation="扫描目录",
                    file_path=str(scan_path),
                    details={"error_type": "不是目录"},
                )
                raise ToolError(
                    f"扫描路径不是目录: {self.config.path}",
                    error_code="VALIDATION_ERROR",
                    context=context,
                    suggestions=["指定一个目录路径", "使用父目录路径"],
                )

            # 默认排除的目录
            EXCLUDED_DIRS = {
                ".venv", "venv", "env",  # 虚拟环境
                ".git", ".svn", ".hg",  # 版本控制
                "__pycache__", ".pytest_cache", ".mypy_cache",  # 缓存
                "node_modules", "dist", "build",  # 构建目录
                ".idea", ".vscode",  # IDE配置
                "site-packages",  # Python包目录
            }

            try:
                # 根据配置选择扫描方式
                if self.config.recursive:
                    pattern = "**/*"
                else:
                    pattern = "*"

                # 扫描文件
                for file_path in scan_path.glob(pattern):
                    # 检查是否在排除的目录中
                    if any(excluded in file_path.parts for excluded in EXCLUDED_DIRS):
                        continue

                    # 只处理文件，跳过目录
                    if not file_path.is_file():
                        continue

                    # 检查是否应该包含此文件
                    if not self._should_include_file(file_path):
                        continue

                    # 获取文件大小
                    file_size = file_path.stat().st_size

                    # 创建文件信息对象
                    files.append(FileInfo(path=file_path, size=file_size))

                logfire.info(f"扫描完成，找到 {len(files)} 个文件")
                return files

            except PermissionError:
                context = ErrorContext(
                    operation="扫描目录",
                    file_path=str(scan_path),
                    details={"error_type": "权限不足"},
                )
                raise ToolError(
                    f"无权限访问目录: {self.config.path}",
                    error_code="PERMISSION_DENIED",
                    context=context,
                    suggestions=[
                        "检查目录权限设置",
                        "尝试使用管理员权限运行",
                        "选择有权限访问的目录",
                    ],
                )
            except OSError as e:
                context = ErrorContext(
                    operation="扫描目录",
                    file_path=str(scan_path),
                    details={"error_type": "系统错误", "original_error": str(e)},
                )
                raise ToolError(
                    f"扫描目录失败: {self.config.path}",
                    error_code="OPERATION_FAILED",
                    context=context,
                    original_error=e,
                    suggestions=[
                        "检查磁盘空间是否充足",
                        "检查文件系统是否正常",
                        "尝试重新运行命令",
                    ],
                )

    def find_duplicates(self) -> list[DuplicateGroup]:
        """查找重复文件.

        返回：DuplicateGroup对象列表，每个组包含重复的文件
        """
        try:
            with logfire.span("find_duplicates"):
                logfire.info("开始重复文件检测")

                # 第一步：扫描所有文件
                all_files = self._scan_files()
                if not all_files:
                    logfire.info("没有找到符合条件的文件")
                    return []

                # 第二步：按文件大小分组
                size_groups = self._group_files_by_size(all_files)
                # 过滤掉只有一个文件的大小组
                potential_duplicates = {
                    size: files for size, files in size_groups.items() if len(files) > 1
                }
                logfire.info(
                    f"按大小分组后，{len(potential_duplicates)} 个大小组"
                    "可能包含重复文件"
                )

                # 第三步：组装哈希任务
                all_files_to_hash = self._collect_files_to_hash(potential_duplicates)
                logfire.info(f"需要计算 {len(all_files_to_hash)} 个文件的哈希值")

                # 第四步：批量计算哈希并分组
                size_hash_groups = self._group_files_by_hash(all_files_to_hash)

                # 第五步：组装最终重复组
                duplicate_groups = self._assemble_duplicate_groups(size_hash_groups)

                # 按可节省空间排序（从大到小）
                duplicate_groups.sort(key=lambda x: x.potential_save, reverse=True)
                logfire.info(f"检测完成，发现 {len(duplicate_groups)} 组重复文件")
                return duplicate_groups
        except ToolError:
            # ToolError 直接传播
            raise
        except Exception as e:
            logfire.error(f"重复文件检测失败: {str(e)}")
            context = ErrorContext(
                operation="重复文件检测",
                details={"error_type": "未知错误", "original_error": str(e)},
            )
            raise ToolError(
                "重复文件检测失败",
                error_code="GENERAL_ERROR",
                context=context,
                original_error=e,
                suggestions=[
                    "检查输入参数是否正确",
                    "确认目录权限设置",
                    "尝试重新运行命令",
                    "如果问题持续，请报告此错误",
                ],
            )

    def _group_files_by_size(
        self, all_files: list["FileInfo"]
    ) -> dict[int, list["FileInfo"]]:
        """按文件大小分组."""
        groups = defaultdict(list)
        for file_info in all_files:
            groups[file_info.size].append(file_info)
        return groups

    def _collect_files_to_hash(
        self, potential_duplicates: dict[int, list["FileInfo"]]
    ) -> list[tuple[int, "FileInfo"]]:
        """组装需要计算哈希的文件列表."""
        all_files_to_hash = []
        for file_size, files in potential_duplicates.items():
            all_files_to_hash.extend([(file_size, file_info) for file_info in files])
        return all_files_to_hash

    def _group_files_by_hash(
        self, all_files_to_hash: list[tuple[int, FileInfo]]
    ) -> dict[int, dict[str, list[FileInfo]]]:
        """批量计算哈希并按大小和哈希分组."""

        def calculate_hash_for_file(
            file_data: tuple[int, FileInfo],
        ) -> Optional[tuple[int, FileInfo, str]]:
            file_size, file_info = file_data
            try:
                file_hash = self._calculate_file_hash(file_info.path)
                file_info.hash = file_hash
                return (file_size, file_info, file_hash)
            except Exception as e:
                logfire.warning(f"跳过文件 {file_info.path}: {str(e)}")
                return None

        size_hash_groups: dict[int, dict[str, list[FileInfo]]] = defaultdict(
            lambda: defaultdict(list)
        )
        results = process_with_progress(
            all_files_to_hash,
            calculate_hash_for_file,
            label="计算文件哈希值",
            threshold=100,
        )
        for result in results:
            if result is not None:
                file_size, file_info, file_hash = result
                size_hash_groups[file_size][file_hash].append(file_info)
        # 转为常规 dict 返回，保证类型一致
        return {size: dict(hash_group) for size, hash_group in size_hash_groups.items()}

    def _assemble_duplicate_groups(
        self, size_hash_groups: dict[int, dict[str, list["FileInfo"]]]
    ) -> list["DuplicateGroup"]:
        """组装最终的重复文件组."""
        duplicate_groups = []
        for file_size, hash_groups in size_hash_groups.items():
            for file_hash, duplicate_files in hash_groups.items():
                if len(duplicate_files) > 1:
                    potential_save = file_size * (len(duplicate_files) - 1)
                    duplicate_group = DuplicateGroup(
                        hash=file_hash,
                        size=file_size,
                        count=len(duplicate_files),
                        files=[f.path for f in duplicate_files],
                        potential_save=potential_save,
                    )
                    duplicate_groups.append(duplicate_group)
        return duplicate_groups


def format_size(size_bytes: int) -> str:
    """将字节数转换为人类可读的文件大小格式.

    复用file_tool.py中的函数逻辑
    """
    if size_bytes == 0:
        return "0 B"

    units = ["B", "KB", "MB", "GB"]
    unit_index = 0
    size = float(size_bytes)

    while size >= 1024 and unit_index < len(units) - 1:
        size /= 1024
        unit_index += 1

    return f"{size:.1f} {units[unit_index]}"


def display_duplicate_results(
    duplicate_groups: list[DuplicateGroup],
    scan_path: str,
    total_files: int,
    recursive: bool,
    show_commands: bool = False,
    ai_analyses: Optional[dict[str, Any]] = None,
) -> None:    """友好展示重复文件检测结果.

    参数：
        duplicate_groups - 重复文件组列表
        scan_path - 扫描路径
        total_files - 扫描的文件总数
        recursive - 是否递归扫描
        show_commands - 是否显示删除建议命令
        ai_analyses - AI分析结果（可选）
    """
    # 输出扫描信息
    click.echo(f"扫描目录: {scan_path}")
    click.echo(f"扫描模式: {'递归扫描' if recursive else '仅顶层目录'}")
    click.echo(f"文件总数: {total_files:,} 个")
    click.echo("━" * 60)

    # 如果没有重复文件
    if not duplicate_groups:
        click.echo("🎉 未发现重复文件！")
        return

    # 显示重复文件组
    click.echo(f"发现 {len(duplicate_groups)} 组重复文件：\n")

    total_duplicates = 0
    total_save_space = 0

    for index, group in enumerate(duplicate_groups, 1):
        total_duplicates += group.count
        total_save_space += group.potential_save

        # 显示组标题
        click.echo(
            f"【第 {index} 组】{group.count} 个文件, "
            f"每个 {format_size(group.size)}, "
            f"可节省 {format_size(group.potential_save)}"
        )

        # 显示文件列表
        for file_path in group.files:
            click.echo(f"  • {file_path}")

        # 显示AI分析结果（如果有）
        if ai_analyses and group.hash in ai_analyses:
            analysis = ai_analyses[group.hash]
            click.echo("\n  🤖 AI分析结果：")
            for line in analysis.split('\n'):
                if line.strip():
                    click.echo(f"     {line}")

        # 如果需要显示删除命令建议
        if show_commands and len(group.files) > 1:
            # 如果有AI分析结果，使用AI的建议
            if ai_analyses and group.hash in ai_analyses:
                analysis_dict = ai_analyses.get(f"{group.hash}_data", {})
                keep_file = analysis_dict.get("recommended_keep")
                if keep_file:
                    click.echo(f"\n  💡 AI建议保留: {keep_file}")
                    files_to_delete = [f for f in group.files if str(f) != str(keep_file)]
                else:
                    click.echo(f"\n  💡 建议保留: {group.files[0]}")
                    files_to_delete = group.files[1:]
            else:
                click.echo(f"\n  💡 建议保留: {group.files[0]}")
                files_to_delete = group.files[1:]

            click.echo("  🗑️  可删除命令:")
            for file_path in files_to_delete:
                click.echo(f"     rm '{file_path}'")

        click.echo()  # 空行分隔

    # 显示总结统计
    click.echo("━" * 60)
    click.echo(
        f"总计：{total_duplicates} 个重复文件，"
        f"可节省 {format_size(total_save_space)} 空间"
    )

    if show_commands:
        click.echo("\n⚠️  警告：删除文件前请确认重要性，建议先备份！")


def _prepare_duplicate_config(
    ctx: click.Context,
    path: str,
    recursive: Optional[bool],
    no_recursive: bool,
    min_size: Optional[int],
    extension: tuple[str, ...],
) -> DuplicateConfig:
    """准备重复文件检测配置.

    从命令行参数和配置文件中获取配置，并处理默认值和冲突。
    """
    # 获取配置
    config = ctx.obj.get("config") if ctx.obj else None

    # 应用配置文件的默认值（命令行参数优先）
    if config and config.duplicates:
        # 递归选项
        if recursive is None and not no_recursive:
            recursive = config.duplicates.recursive
        # 最小文件大小
        if min_size is None:
            min_size = config.duplicates.min_size
        # 文件扩展名
        if not extension and config.duplicates.extensions:
            extension = tuple(config.duplicates.extensions)

    # 设置默认值
    if recursive is None and not no_recursive:
        recursive = True
    if min_size is None:
        min_size = 1

    # 处理递归选项冲突
    if no_recursive:
        recursive = False

    # 转换扩展名列表
    extensions = list(extension) if extension else None

    # 创建配置对象
    return DuplicateConfig(
        path=path, recursive=recursive, min_size=min_size, extensions=extensions
    )


def _execute_duplicate_finder(
    finder: DuplicateFinder,
) -> tuple[list[DuplicateGroup], list[FileInfo]]:
    """执行重复文件检测.

    返回检测结果和扫描的文件列表。
    """
    # 执行检测
    duplicate_groups = finder.find_duplicates()

    # 计算扫描的总文件数（用于统计显示）
    all_files = finder._scan_files()

    return duplicate_groups, all_files


def _handle_formatted_output(
    duplicate_groups: list[DuplicateGroup], format_type: str
) -> None:
    """处理格式化输出（JSON/CSV）."""
    # 计算总的节省空间
    total_save_space = sum(group.potential_save for group in duplicate_groups)

    # 构建格式化数据
    groups_data = []
    for group in duplicate_groups:
        groups_data.append(
            {
                "hash": group.hash,
                "size": group.size,
                "count": group.count,
                "files": [str(f) for f in group.files],
            }
        )

    # 创建数据模型
    data = DuplicateData(
        total_groups=len(duplicate_groups),
        total_size_saved=total_save_space,
        groups=groups_data,
    )

    # 格式化输出
    output = format_output(data, format_type)
    click.echo(output)



# 新增：执行AI分析的异步函数
async def _perform_ai_analysis(
    duplicate_groups: list[DuplicateGroup],
    ai_config: AIConfig,
) -> dict[str, Any]:
    """对重复文件组执行AI版本分析.

    Args:
        duplicate_groups: 重复文件组列表
        ai_config: AI配置

    Returns:
        AI分析结果字典
    """
    analyzer = VersionAnalyzer(ai_config)
    analyses = {}

    for group in duplicate_groups:
        try:
            with logfire.span("ai_version_analysis", attributes={
                "file_count": group.count,
                "file_size": group.size,
            }):
                # 执行AI分析
                analysis = await analyzer.analyze_with_ai(group.files)

                # 格式化分析结果
                formatted_result = analyzer.format_analysis_result(analysis)
                analyses[group.hash] = formatted_result

                # 保存原始数据用于命令建议
                analyses[f"{group.hash}_data"] = {
                    "recommended_keep": analysis.recommended_keep,
                    "confidence": analysis.confidence,
                }

                logfire.info(f"AI分析完成：{group.count}个文件")
        except Exception as e:
            logfire.error(f"AI分析失败: {e}")
            # 继续处理其他组

    return analyses

@command()
@argument("path", type=click.Path(exists=True), default=".")
@option("-r", "--recursive", is_flag=True, default=None, help="递归扫描子目录")
@option("-n", "--no-recursive", is_flag=True, help="仅扫描顶层目录，不递归")
@option("-s", "--min-size", type=int, default=None, help="最小文件大小（字节）")
@option(
    "-e",
    "--extension",
    multiple=True,
    help="指定文件扩展名（可多次使用），如：-e .jpg -e .png",
)
@option("--show-commands", is_flag=True, help="显示删除重复文件的建议命令")
@option("--ai-analyze", is_flag=True, help="使用AI分析文件版本关系")
@option(
    "--format",
    type=click.Choice(["plain", "json", "csv"], case_sensitive=False),
    default=None,
    help="输出格式（plain/json/csv）",
)
@pass_context
def duplicates_cmd(
    ctx: click.Context,
    path: str,
    recursive: Optional[bool],
    no_recursive: bool,
    min_size: Optional[int],
    extension: tuple[str, ...],
    show_commands: bool,
    ai_analyze: bool,
    format: Optional[str],
) -> None:
    """查找指定目录中的重复文件.

    PATH: 要扫描的目录路径（默认为当前目录）

    示例用法：
      tools duplicates .                    # 扫描当前目录
      tools duplicates ~/Downloads -n       # 只扫描Downloads顶层
      tools duplicates . -s 1048576        # 只查找大于1MB的文件
      tools duplicates . -e .jpg -e .png   # 只查找图片文件
      tools duplicates . --show-commands   # 显示删除建议
      tools duplicates . --ai-analyze      # 使用AI分析版本关系
      tools duplicates . --format json     # JSON格式输出
    """
    # 获取配置
    config = ctx.obj.get("config") if ctx.obj else None

    # 设置默认输出格式
    if format is None:
        format = config.format if config else "plain"

    try:
        # 准备配置
        duplicate_config = _prepare_duplicate_config(
            ctx, path, recursive, no_recursive, min_size, extension
        )

        # 创建检测器
        finder = DuplicateFinder(duplicate_config)

        # 显示开始信息（仅在plain格式时显示）
        if format == "plain":
            click.echo("🔍 开始扫描重复文件...")

        # 执行检测
        duplicate_groups, all_files = _execute_duplicate_finder(finder)

        # 执行AI分析（如果启用）
        ai_analyses = None
        if ai_analyze and duplicate_groups:
            # 获取AI配置
            ai_config = AIConfig()
            if not ai_config.enabled:
                click.echo("\n⚠️  AI功能未启用，请设置 DEEPSEEK_API_KEY 环境变量")
            else:
                click.echo("\n🤖 正在进行AI版本分析...")
                # 运行异步AI分析
                ai_analyses = asyncio.run(_perform_ai_analysis(duplicate_groups, ai_config))
                click.echo("✅ AI分析完成\n")


        # 根据格式选择输出方式
        if format != "plain":
            # 如果有AI分析，添加到JSON/CSV输出
            if ai_analyses:
                # 增强格式化数据
                groups_data = []
                for group in duplicate_groups:
                    group_data = {
                        "hash": group.hash,
                        "size": group.size,
                        "count": group.count,
                        "files": [str(f) for f in group.files],
                    }
                    # 添加AI分析结果
                    if group.hash in ai_analyses:
                        data_key = f"{group.hash}_data"
                        if data_key in ai_analyses:
                            group_data["ai_recommendation"] = str(ai_analyses[data_key].get("recommended_keep", ""))
                            group_data["ai_confidence"] = ai_analyses[data_key].get("confidence", 0.0)
                    groups_data.append(group_data)

                # 创建增强的数据模型
                total_save_space = sum(group.potential_save for group in duplicate_groups)
                data = DuplicateData(
                    total_groups=len(duplicate_groups),
                    total_size_saved=total_save_space,
                    groups=groups_data,
                )
                output = format_output(data, format)
                click.echo(output)
            else:
                _handle_formatted_output(duplicate_groups, format)
        else:
            # 保持原有的纯文本输出方式
            display_duplicate_results(
                duplicate_groups=duplicate_groups,
                scan_path=os.path.abspath(path),
                total_files=len(all_files),
                recursive=duplicate_config.recursive,
                show_commands=show_commands,
            )

        # 记录到操作历史
        from ..utils.smart_interactive import operation_history

        # 计算总的节省空间
        total_save_space = sum(group.potential_save for group in duplicate_groups)

        operation_history.add(
            "duplicates",
            {
                "path": path,
                "recursive": duplicate_config.recursive,
                "min_size": duplicate_config.min_size,
                "extensions": duplicate_config.extensions,
                "format": format,
                "ai_analyze": ai_analyze,  # 记录是否使用AI
            },
            {
                "scanned_files": len(all_files),
                "duplicate_groups": len(duplicate_groups),
                "space_saved": total_save_space,
            },
        )

    except ToolError:
        # ToolError 会被 handle_errors 装饰器自动处理
        raise
    except click.ClickException:
        # Click异常直接传播
        raise
    except Exception as e:
        # 其他异常转换为ToolError
        logfire.error(f"重复文件检测失败: {str(e)}")
        context = ErrorContext(
            operation="重复文件检测",
            details={"error_type": "未知错误", "original_error": str(e)},
        )
        raise ToolError(
            "重复文件检测失败",
            error_code="GENERAL_ERROR",
            context=context,
            original_error=e,
            suggestions=[
                "检查输入参数是否正确",
                "确认目录权限设置",
                "尝试重新运行命令",
                "如果问题持续，请报告此错误",
            ],
        )
