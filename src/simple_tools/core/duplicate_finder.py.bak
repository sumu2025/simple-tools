"""é‡å¤æ–‡ä»¶æ£€æµ‹æ¨¡å—."""

import asyncio
import hashlib
import os
from collections import defaultdict
from pathlib import Path
from typing import Any, Optional

import click
import logfire
from pydantic import BaseModel, Field

from simple_tools._typing import argument, command, option, pass_context
from simple_tools.utils.errors import ErrorContext, ToolError

from ..ai.config import AIConfig
from ..ai.version_analyzer import VersionAnalyzer
from ..utils.formatter import DuplicateData, format_output
from ..utils.progress import process_with_progress


class DuplicateConfig(BaseModel):
    """é‡å¤æ–‡ä»¶æ£€æµ‹é…ç½®."""

    path: str = Field(..., description="æ‰«æè·¯å¾„")
    recursive: bool = Field(True, description="æ˜¯å¦é€’å½’æ‰«æå­ç›®å½•")
    min_size: int = Field(1, description="æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    extensions: Optional[list[str]] = Field(None, description="æŒ‡å®šæ–‡ä»¶æ‰©å±•å")


class FileInfo(BaseModel):
    """æ–‡ä»¶ä¿¡æ¯."""

    path: Path = Field(..., description="æ–‡ä»¶è·¯å¾„")
    size: int = Field(..., description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    hash: Optional[str] = Field(None, description="æ–‡ä»¶MD5å“ˆå¸Œå€¼")


class DuplicateGroup(BaseModel):
    """é‡å¤æ–‡ä»¶ç»„."""

    hash: str = Field(..., description="æ–‡ä»¶MD5å“ˆå¸Œå€¼")
    size: int = Field(..., description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    count: int = Field(..., description="é‡å¤æ–‡ä»¶æ•°é‡")
    files: list[Path] = Field(..., description="é‡å¤æ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    potential_save: int = Field(..., description="å¯èŠ‚çœç©ºé—´ï¼ˆå­—èŠ‚ï¼‰ä¿ç•™ä¸€ä¸ªæ–‡ä»¶")


class DuplicateFinder:
    """é‡å¤æ–‡ä»¶æ£€æµ‹å™¨."""

    def __init__(self, config: DuplicateConfig):
        """åˆå§‹åŒ–é‡å¤æ–‡ä»¶æ£€æµ‹å™¨.

        å‚æ•°ï¼šconfig - æ£€æµ‹é…ç½®å¯¹è±¡
        """
        self.config = config
        logfire.info("åˆå§‹åŒ–é‡å¤æ–‡ä»¶æ£€æµ‹å™¨", attributes={"config": config.model_dump()})

    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼ˆåˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜é—®é¢˜ï¼‰.

        å‚æ•°ï¼šfile_path - æ–‡ä»¶è·¯å¾„å¯¹è±¡
        è¿”å›ï¼šMD5å“ˆå¸Œå€¼å­—ç¬¦ä¸²
        """
        hash_md5 = hashlib.md5()

        try:
            with open(file_path, "rb") as f:
                # æ¯æ¬¡è¯»å–8KBï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except PermissionError:
            context = ErrorContext(
                operation="è®¡ç®—æ–‡ä»¶å“ˆå¸Œ",
                file_path=str(file_path),
                details={"error_type": "æƒé™ä¸è¶³"},
            )
            raise ToolError(
                f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}",
                error_code="PERMISSION_DENIED",
                context=context,
                suggestions=[
                    "æ£€æŸ¥æ–‡ä»¶æƒé™è®¾ç½®",
                    "å°è¯•ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œ",
                    "ç¡®è®¤æ–‡ä»¶æœªè¢«å…¶ä»–ç¨‹åºå ç”¨",
                ],
            )
        except FileNotFoundError:
            context = ErrorContext(
                operation="è®¡ç®—æ–‡ä»¶å“ˆå¸Œ",
                file_path=str(file_path),
                details={"error_type": "æ–‡ä»¶ä¸å­˜åœ¨"},
            )
            raise ToolError(
                f"æ–‡ä»¶ä¸å­˜åœ¨ {file_path}",
                error_code="FILE_NOT_FOUND",
                context=context,
                suggestions=["é‡æ–°è¿è¡Œæ‰«æ", "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ç§»åŠ¨æˆ–åˆ é™¤"],
            )
        except OSError as e:
            context = ErrorContext(
                operation="è®¡ç®—æ–‡ä»¶å“ˆå¸Œ",
                file_path=str(file_path),
                details={"error_type": "ç³»ç»Ÿé”™è¯¯", "original_error": str(e)},
            )
            raise ToolError(
                f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}",
                error_code="OPERATION_FAILED",
                context=context,
                original_error=e,
                suggestions=[
                    "æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³",
                    "æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿæ˜¯å¦æ­£å¸¸",
                    "å°è¯•é‡æ–°è¿è¡Œå‘½ä»¤",
                ],
            )
        except Exception as e:
            logfire.error(
                f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {file_path}", attributes={"error": str(e)}
            )
            context = ErrorContext(
                operation="è®¡ç®—æ–‡ä»¶å“ˆå¸Œ",
                file_path=str(file_path),
                details={"error_type": "æœªçŸ¥é”™è¯¯", "original_error": str(e)},
            )
            raise ToolError(
                f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}",
                error_code="GENERAL_ERROR",
                context=context,
                original_error=e,
                suggestions=[
                    "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå",
                    "å°è¯•é‡æ–°è¿è¡Œå‘½ä»¤",
                    "å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æŠ¥å‘Šæ­¤é”™è¯¯",
                ],
            )

    def _should_include_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥åŒ…å«åœ¨æ£€æµ‹èŒƒå›´å†….

        å‚æ•°ï¼šfile_path - æ–‡ä»¶è·¯å¾„å¯¹è±¡
        è¿”å›ï¼šå¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºåº”è¯¥åŒ…å«
        """
        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
        try:
            file_size = file_path.stat().st_size
            if file_size < self.config.min_size:
                return False
        except OSError:
            return False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦åœ¨æŒ‡å®šåˆ—è¡¨ä¸­
        if self.config.extensions:
            file_ext = file_path.suffix.lower()
            return file_ext in [ext.lower() for ext in self.config.extensions]

        return True

    def _scan_files(self) -> list[FileInfo]:
        """æ‰«æç›®å½•è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ä¿¡æ¯.

        è¿”å›ï¼šFileInfoå¯¹è±¡åˆ—è¡¨
        """
        with logfire.span("scan_files", attributes={"path": self.config.path}):
            files = []
            scan_path = Path(self.config.path)

            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not scan_path.exists():
                context = ErrorContext(
                    operation="æ‰«æç›®å½•",
                    file_path=str(scan_path),
                    details={"error_type": "è·¯å¾„ä¸å­˜åœ¨"},
                )
                raise ToolError(
                    f"æ‰«æè·¯å¾„ä¸å­˜åœ¨: {self.config.path}",
                    error_code="FILE_NOT_FOUND",
                    context=context,
                    suggestions=[
                        "æ£€æŸ¥è·¯å¾„æ‹¼å†™æ˜¯å¦æ­£ç¡®",
                        "ç¡®è®¤ç›®å½•æ˜¯å¦å­˜åœ¨",
                        "ä½¿ç”¨ç»å¯¹è·¯å¾„é‡è¯•",
                    ],
                )

            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
            if not scan_path.is_dir():
                context = ErrorContext(
                    operation="æ‰«æç›®å½•",
                    file_path=str(scan_path),
                    details={"error_type": "ä¸æ˜¯ç›®å½•"},
                )
                raise ToolError(
                    f"æ‰«æè·¯å¾„ä¸æ˜¯ç›®å½•: {self.config.path}",
                    error_code="VALIDATION_ERROR",
                    context=context,
                    suggestions=["æŒ‡å®šä¸€ä¸ªç›®å½•è·¯å¾„", "ä½¿ç”¨çˆ¶ç›®å½•è·¯å¾„"],
                )

            # é»˜è®¤æ’é™¤çš„ç›®å½•
            EXCLUDED_DIRS = {
                ".venv", "venv", "env",  # è™šæ‹Ÿç¯å¢ƒ
                ".git", ".svn", ".hg",  # ç‰ˆæœ¬æ§åˆ¶
                "__pycache__", ".pytest_cache", ".mypy_cache",  # ç¼“å­˜
                "node_modules", "dist", "build",  # æ„å»ºç›®å½•
                ".idea", ".vscode",  # IDEé…ç½®
                "site-packages",  # PythonåŒ…ç›®å½•
            }

            try:
                # æ ¹æ®é…ç½®é€‰æ‹©æ‰«ææ–¹å¼
                if self.config.recursive:
                    pattern = "**/*"
                else:
                    pattern = "*"

                # æ‰«ææ–‡ä»¶
                for file_path in scan_path.glob(pattern):
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤çš„ç›®å½•ä¸­
                    if any(excluded in file_path.parts for excluded in EXCLUDED_DIRS):
                        continue

                    # åªå¤„ç†æ–‡ä»¶ï¼Œè·³è¿‡ç›®å½•
                    if not file_path.is_file():
                        continue

                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«æ­¤æ–‡ä»¶
                    if not self._should_include_file(file_path):
                        continue

                    # è·å–æ–‡ä»¶å¤§å°
                    file_size = file_path.stat().st_size

                    # åˆ›å»ºæ–‡ä»¶ä¿¡æ¯å¯¹è±¡
                    files.append(FileInfo(path=file_path, size=file_size))

                logfire.info(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
                return files

            except PermissionError:
                context = ErrorContext(
                    operation="æ‰«æç›®å½•",
                    file_path=str(scan_path),
                    details={"error_type": "æƒé™ä¸è¶³"},
                )
                raise ToolError(
                    f"æ— æƒé™è®¿é—®ç›®å½•: {self.config.path}",
                    error_code="PERMISSION_DENIED",
                    context=context,
                    suggestions=[
                        "æ£€æŸ¥ç›®å½•æƒé™è®¾ç½®",
                        "å°è¯•ä½¿ç”¨ç®¡ç†å‘˜æƒé™è¿è¡Œ",
                        "é€‰æ‹©æœ‰æƒé™è®¿é—®çš„ç›®å½•",
                    ],
                )
            except OSError as e:
                context = ErrorContext(
                    operation="æ‰«æç›®å½•",
                    file_path=str(scan_path),
                    details={"error_type": "ç³»ç»Ÿé”™è¯¯", "original_error": str(e)},
                )
                raise ToolError(
                    f"æ‰«æç›®å½•å¤±è´¥: {self.config.path}",
                    error_code="OPERATION_FAILED",
                    context=context,
                    original_error=e,
                    suggestions=[
                        "æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³",
                        "æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿæ˜¯å¦æ­£å¸¸",
                        "å°è¯•é‡æ–°è¿è¡Œå‘½ä»¤",
                    ],
                )

    def find_duplicates(self) -> list[DuplicateGroup]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶.

        è¿”å›ï¼šDuplicateGroupå¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªç»„åŒ…å«é‡å¤çš„æ–‡ä»¶
        """
        try:
            with logfire.span("find_duplicates"):
                logfire.info("å¼€å§‹é‡å¤æ–‡ä»¶æ£€æµ‹")

                # ç¬¬ä¸€æ­¥ï¼šæ‰«ææ‰€æœ‰æ–‡ä»¶
                all_files = self._scan_files()
                if not all_files:
                    logfire.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
                    return []

                # ç¬¬äºŒæ­¥ï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
                size_groups = self._group_files_by_size(all_files)
                # è¿‡æ»¤æ‰åªæœ‰ä¸€ä¸ªæ–‡ä»¶çš„å¤§å°ç»„
                potential_duplicates = {
                    size: files for size, files in size_groups.items() if len(files) > 1
                }
                logfire.info(
                    f"æŒ‰å¤§å°åˆ†ç»„åï¼Œ{len(potential_duplicates)} ä¸ªå¤§å°ç»„"
                    "å¯èƒ½åŒ…å«é‡å¤æ–‡ä»¶"
                )

                # ç¬¬ä¸‰æ­¥ï¼šç»„è£…å“ˆå¸Œä»»åŠ¡
                all_files_to_hash = self._collect_files_to_hash(potential_duplicates)
                logfire.info(f"éœ€è¦è®¡ç®— {len(all_files_to_hash)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼")

                # ç¬¬å››æ­¥ï¼šæ‰¹é‡è®¡ç®—å“ˆå¸Œå¹¶åˆ†ç»„
                size_hash_groups = self._group_files_by_hash(all_files_to_hash)

                # ç¬¬äº”æ­¥ï¼šç»„è£…æœ€ç»ˆé‡å¤ç»„
                duplicate_groups = self._assemble_duplicate_groups(size_hash_groups)

                # æŒ‰å¯èŠ‚çœç©ºé—´æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
                duplicate_groups.sort(key=lambda x: x.potential_save, reverse=True)
                logfire.info(f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(duplicate_groups)} ç»„é‡å¤æ–‡ä»¶")
                return duplicate_groups
        except ToolError:
            # ToolError ç›´æ¥ä¼ æ’­
            raise
        except Exception as e:
            logfire.error(f"é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥: {str(e)}")
            context = ErrorContext(
                operation="é‡å¤æ–‡ä»¶æ£€æµ‹",
                details={"error_type": "æœªçŸ¥é”™è¯¯", "original_error": str(e)},
            )
            raise ToolError(
                "é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥",
                error_code="GENERAL_ERROR",
                context=context,
                original_error=e,
                suggestions=[
                    "æ£€æŸ¥è¾“å…¥å‚æ•°æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤ç›®å½•æƒé™è®¾ç½®",
                    "å°è¯•é‡æ–°è¿è¡Œå‘½ä»¤",
                    "å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æŠ¥å‘Šæ­¤é”™è¯¯",
                ],
            )

    def _group_files_by_size(
        self, all_files: list["FileInfo"]
    ) -> dict[int, list["FileInfo"]]:
        """æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„."""
        groups = defaultdict(list)
        for file_info in all_files:
            groups[file_info.size].append(file_info)
        return groups

    def _collect_files_to_hash(
        self, potential_duplicates: dict[int, list["FileInfo"]]
    ) -> list[tuple[int, "FileInfo"]]:
        """ç»„è£…éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶åˆ—è¡¨."""
        all_files_to_hash = []
        for file_size, files in potential_duplicates.items():
            all_files_to_hash.extend([(file_size, file_info) for file_info in files])
        return all_files_to_hash

    def _group_files_by_hash(
        self, all_files_to_hash: list[tuple[int, FileInfo]]
    ) -> dict[int, dict[str, list[FileInfo]]]:
        """æ‰¹é‡è®¡ç®—å“ˆå¸Œå¹¶æŒ‰å¤§å°å’Œå“ˆå¸Œåˆ†ç»„."""

        def calculate_hash_for_file(
            file_data: tuple[int, FileInfo],
        ) -> Optional[tuple[int, FileInfo, str]]:
            file_size, file_info = file_data
            try:
                file_hash = self._calculate_file_hash(file_info.path)
                file_info.hash = file_hash
                return (file_size, file_info, file_hash)
            except Exception as e:
                logfire.warning(f"è·³è¿‡æ–‡ä»¶ {file_info.path}: {str(e)}")
                return None

        size_hash_groups: dict[int, dict[str, list[FileInfo]]] = defaultdict(
            lambda: defaultdict(list)
        )
        results = process_with_progress(
            all_files_to_hash,
            calculate_hash_for_file,
            label="è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼",
            threshold=100,
        )
        for result in results:
            if result is not None:
                file_size, file_info, file_hash = result
                size_hash_groups[file_size][file_hash].append(file_info)
        # è½¬ä¸ºå¸¸è§„ dict è¿”å›ï¼Œä¿è¯ç±»å‹ä¸€è‡´
        return {size: dict(hash_group) for size, hash_group in size_hash_groups.items()}

    def _assemble_duplicate_groups(
        self, size_hash_groups: dict[int, dict[str, list["FileInfo"]]]
    ) -> list["DuplicateGroup"]:
        """ç»„è£…æœ€ç»ˆçš„é‡å¤æ–‡ä»¶ç»„."""
        duplicate_groups = []
        for file_size, hash_groups in size_hash_groups.items():
            for file_hash, duplicate_files in hash_groups.items():
                if len(duplicate_files) > 1:
                    potential_save = file_size * (len(duplicate_files) - 1)
                    duplicate_group = DuplicateGroup(
                        hash=file_hash,
                        size=file_size,
                        count=len(duplicate_files),
                        files=[f.path for f in duplicate_files],
                        potential_save=potential_save,
                    )
                    duplicate_groups.append(duplicate_group)
        return duplicate_groups


def format_size(size_bytes: int) -> str:
    """å°†å­—èŠ‚æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ–‡ä»¶å¤§å°æ ¼å¼.

    å¤ç”¨file_tool.pyä¸­çš„å‡½æ•°é€»è¾‘
    """
    if size_bytes == 0:
        return "0 B"

    units = ["B", "KB", "MB", "GB"]
    unit_index = 0
    size = float(size_bytes)

    while size >= 1024 and unit_index < len(units) - 1:
        size /= 1024
        unit_index += 1

    return f"{size:.1f} {units[unit_index]}"


def display_duplicate_results(
    duplicate_groups: list[DuplicateGroup],
    scan_path: str,
    total_files: int,
    recursive: bool,
    show_commands: bool = False,
    ai_analyses: Optional[dict[str, Any]] = None,
) -> None:    """å‹å¥½å±•ç¤ºé‡å¤æ–‡ä»¶æ£€æµ‹ç»“æœ.

    å‚æ•°ï¼š
        duplicate_groups - é‡å¤æ–‡ä»¶ç»„åˆ—è¡¨
        scan_path - æ‰«æè·¯å¾„
        total_files - æ‰«æçš„æ–‡ä»¶æ€»æ•°
        recursive - æ˜¯å¦é€’å½’æ‰«æ
        show_commands - æ˜¯å¦æ˜¾ç¤ºåˆ é™¤å»ºè®®å‘½ä»¤
        ai_analyses - AIåˆ†æç»“æœï¼ˆå¯é€‰ï¼‰
    """
    # è¾“å‡ºæ‰«æä¿¡æ¯
    click.echo(f"æ‰«æç›®å½•: {scan_path}")
    click.echo(f"æ‰«ææ¨¡å¼: {'é€’å½’æ‰«æ' if recursive else 'ä»…é¡¶å±‚ç›®å½•'}")
    click.echo(f"æ–‡ä»¶æ€»æ•°: {total_files:,} ä¸ª")
    click.echo("â”" * 60)

    # å¦‚æœæ²¡æœ‰é‡å¤æ–‡ä»¶
    if not duplicate_groups:
        click.echo("ğŸ‰ æœªå‘ç°é‡å¤æ–‡ä»¶ï¼")
        return

    # æ˜¾ç¤ºé‡å¤æ–‡ä»¶ç»„
    click.echo(f"å‘ç° {len(duplicate_groups)} ç»„é‡å¤æ–‡ä»¶ï¼š\n")

    total_duplicates = 0
    total_save_space = 0

    for index, group in enumerate(duplicate_groups, 1):
        total_duplicates += group.count
        total_save_space += group.potential_save

        # æ˜¾ç¤ºç»„æ ‡é¢˜
        click.echo(
            f"ã€ç¬¬ {index} ç»„ã€‘{group.count} ä¸ªæ–‡ä»¶, "
            f"æ¯ä¸ª {format_size(group.size)}, "
            f"å¯èŠ‚çœ {format_size(group.potential_save)}"
        )

        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        for file_path in group.files:
            click.echo(f"  â€¢ {file_path}")

        # æ˜¾ç¤ºAIåˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if ai_analyses and group.hash in ai_analyses:
            analysis = ai_analyses[group.hash]
            click.echo("\n  ğŸ¤– AIåˆ†æç»“æœï¼š")
            for line in analysis.split('\n'):
                if line.strip():
                    click.echo(f"     {line}")

        # å¦‚æœéœ€è¦æ˜¾ç¤ºåˆ é™¤å‘½ä»¤å»ºè®®
        if show_commands and len(group.files) > 1:
            # å¦‚æœæœ‰AIåˆ†æç»“æœï¼Œä½¿ç”¨AIçš„å»ºè®®
            if ai_analyses and group.hash in ai_analyses:
                analysis_dict = ai_analyses.get(f"{group.hash}_data", {})
                keep_file = analysis_dict.get("recommended_keep")
                if keep_file:
                    click.echo(f"\n  ğŸ’¡ AIå»ºè®®ä¿ç•™: {keep_file}")
                    files_to_delete = [f for f in group.files if str(f) != str(keep_file)]
                else:
                    click.echo(f"\n  ğŸ’¡ å»ºè®®ä¿ç•™: {group.files[0]}")
                    files_to_delete = group.files[1:]
            else:
                click.echo(f"\n  ğŸ’¡ å»ºè®®ä¿ç•™: {group.files[0]}")
                files_to_delete = group.files[1:]

            click.echo("  ğŸ—‘ï¸  å¯åˆ é™¤å‘½ä»¤:")
            for file_path in files_to_delete:
                click.echo(f"     rm '{file_path}'")

        click.echo()  # ç©ºè¡Œåˆ†éš”

    # æ˜¾ç¤ºæ€»ç»“ç»Ÿè®¡
    click.echo("â”" * 60)
    click.echo(
        f"æ€»è®¡ï¼š{total_duplicates} ä¸ªé‡å¤æ–‡ä»¶ï¼Œ"
        f"å¯èŠ‚çœ {format_size(total_save_space)} ç©ºé—´"
    )

    if show_commands:
        click.echo("\nâš ï¸  è­¦å‘Šï¼šåˆ é™¤æ–‡ä»¶å‰è¯·ç¡®è®¤é‡è¦æ€§ï¼Œå»ºè®®å…ˆå¤‡ä»½ï¼")


def _prepare_duplicate_config(
    ctx: click.Context,
    path: str,
    recursive: Optional[bool],
    no_recursive: bool,
    min_size: Optional[int],
    extension: tuple[str, ...],
) -> DuplicateConfig:
    """å‡†å¤‡é‡å¤æ–‡ä»¶æ£€æµ‹é…ç½®.

    ä»å‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ä¸­è·å–é…ç½®ï¼Œå¹¶å¤„ç†é»˜è®¤å€¼å’Œå†²çªã€‚
    """
    # è·å–é…ç½®
    config = ctx.obj.get("config") if ctx.obj else None

    # åº”ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
    if config and config.duplicates:
        # é€’å½’é€‰é¡¹
        if recursive is None and not no_recursive:
            recursive = config.duplicates.recursive
        # æœ€å°æ–‡ä»¶å¤§å°
        if min_size is None:
            min_size = config.duplicates.min_size
        # æ–‡ä»¶æ‰©å±•å
        if not extension and config.duplicates.extensions:
            extension = tuple(config.duplicates.extensions)

    # è®¾ç½®é»˜è®¤å€¼
    if recursive is None and not no_recursive:
        recursive = True
    if min_size is None:
        min_size = 1

    # å¤„ç†é€’å½’é€‰é¡¹å†²çª
    if no_recursive:
        recursive = False

    # è½¬æ¢æ‰©å±•ååˆ—è¡¨
    extensions = list(extension) if extension else None

    # åˆ›å»ºé…ç½®å¯¹è±¡
    return DuplicateConfig(
        path=path, recursive=recursive, min_size=min_size, extensions=extensions
    )


def _execute_duplicate_finder(
    finder: DuplicateFinder,
) -> tuple[list[DuplicateGroup], list[FileInfo]]:
    """æ‰§è¡Œé‡å¤æ–‡ä»¶æ£€æµ‹.

    è¿”å›æ£€æµ‹ç»“æœå’Œæ‰«æçš„æ–‡ä»¶åˆ—è¡¨ã€‚
    """
    # æ‰§è¡Œæ£€æµ‹
    duplicate_groups = finder.find_duplicates()

    # è®¡ç®—æ‰«æçš„æ€»æ–‡ä»¶æ•°ï¼ˆç”¨äºç»Ÿè®¡æ˜¾ç¤ºï¼‰
    all_files = finder._scan_files()

    return duplicate_groups, all_files


def _handle_formatted_output(
    duplicate_groups: list[DuplicateGroup], format_type: str
) -> None:
    """å¤„ç†æ ¼å¼åŒ–è¾“å‡ºï¼ˆJSON/CSVï¼‰."""
    # è®¡ç®—æ€»çš„èŠ‚çœç©ºé—´
    total_save_space = sum(group.potential_save for group in duplicate_groups)

    # æ„å»ºæ ¼å¼åŒ–æ•°æ®
    groups_data = []
    for group in duplicate_groups:
        groups_data.append(
            {
                "hash": group.hash,
                "size": group.size,
                "count": group.count,
                "files": [str(f) for f in group.files],
            }
        )

    # åˆ›å»ºæ•°æ®æ¨¡å‹
    data = DuplicateData(
        total_groups=len(duplicate_groups),
        total_size_saved=total_save_space,
        groups=groups_data,
    )

    # æ ¼å¼åŒ–è¾“å‡º
    output = format_output(data, format_type)
    click.echo(output)



# æ–°å¢ï¼šæ‰§è¡ŒAIåˆ†æçš„å¼‚æ­¥å‡½æ•°
async def _perform_ai_analysis(
    duplicate_groups: list[DuplicateGroup],
    ai_config: AIConfig,
) -> dict[str, Any]:
    """å¯¹é‡å¤æ–‡ä»¶ç»„æ‰§è¡ŒAIç‰ˆæœ¬åˆ†æ.

    Args:
        duplicate_groups: é‡å¤æ–‡ä»¶ç»„åˆ—è¡¨
        ai_config: AIé…ç½®

    Returns:
        AIåˆ†æç»“æœå­—å…¸
    """
    analyzer = VersionAnalyzer(ai_config)
    analyses = {}

    for group in duplicate_groups:
        try:
            with logfire.span("ai_version_analysis", attributes={
                "file_count": group.count,
                "file_size": group.size,
            }):
                # æ‰§è¡ŒAIåˆ†æ
                analysis = await analyzer.analyze_with_ai(group.files)

                # æ ¼å¼åŒ–åˆ†æç»“æœ
                formatted_result = analyzer.format_analysis_result(analysis)
                analyses[group.hash] = formatted_result

                # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºå‘½ä»¤å»ºè®®
                analyses[f"{group.hash}_data"] = {
                    "recommended_keep": analysis.recommended_keep,
                    "confidence": analysis.confidence,
                }

                logfire.info(f"AIåˆ†æå®Œæˆï¼š{group.count}ä¸ªæ–‡ä»¶")
        except Exception as e:
            logfire.error(f"AIåˆ†æå¤±è´¥: {e}")
            # ç»§ç»­å¤„ç†å…¶ä»–ç»„

    return analyses

@command()
@argument("path", type=click.Path(exists=True), default=".")
@option("-r", "--recursive", is_flag=True, default=None, help="é€’å½’æ‰«æå­ç›®å½•")
@option("-n", "--no-recursive", is_flag=True, help="ä»…æ‰«æé¡¶å±‚ç›®å½•ï¼Œä¸é€’å½’")
@option("-s", "--min-size", type=int, default=None, help="æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
@option(
    "-e",
    "--extension",
    multiple=True,
    help="æŒ‡å®šæ–‡ä»¶æ‰©å±•åï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰ï¼Œå¦‚ï¼š-e .jpg -e .png",
)
@option("--show-commands", is_flag=True, help="æ˜¾ç¤ºåˆ é™¤é‡å¤æ–‡ä»¶çš„å»ºè®®å‘½ä»¤")
@option("--ai-analyze", is_flag=True, help="ä½¿ç”¨AIåˆ†ææ–‡ä»¶ç‰ˆæœ¬å…³ç³»")
@option(
    "--format",
    type=click.Choice(["plain", "json", "csv"], case_sensitive=False),
    default=None,
    help="è¾“å‡ºæ ¼å¼ï¼ˆplain/json/csvï¼‰",
)
@pass_context
def duplicates_cmd(
    ctx: click.Context,
    path: str,
    recursive: Optional[bool],
    no_recursive: bool,
    min_size: Optional[int],
    extension: tuple[str, ...],
    show_commands: bool,
    ai_analyze: bool,
    format: Optional[str],
) -> None:
    """æŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶.

    PATH: è¦æ‰«æçš„ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰

    ç¤ºä¾‹ç”¨æ³•ï¼š
      tools duplicates .                    # æ‰«æå½“å‰ç›®å½•
      tools duplicates ~/Downloads -n       # åªæ‰«æDownloadsé¡¶å±‚
      tools duplicates . -s 1048576        # åªæŸ¥æ‰¾å¤§äº1MBçš„æ–‡ä»¶
      tools duplicates . -e .jpg -e .png   # åªæŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
      tools duplicates . --show-commands   # æ˜¾ç¤ºåˆ é™¤å»ºè®®
      tools duplicates . --ai-analyze      # ä½¿ç”¨AIåˆ†æç‰ˆæœ¬å…³ç³»
      tools duplicates . --format json     # JSONæ ¼å¼è¾“å‡º
    """
    # è·å–é…ç½®
    config = ctx.obj.get("config") if ctx.obj else None

    # è®¾ç½®é»˜è®¤è¾“å‡ºæ ¼å¼
    if format is None:
        format = config.format if config else "plain"

    try:
        # å‡†å¤‡é…ç½®
        duplicate_config = _prepare_duplicate_config(
            ctx, path, recursive, no_recursive, min_size, extension
        )

        # åˆ›å»ºæ£€æµ‹å™¨
        finder = DuplicateFinder(duplicate_config)

        # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯ï¼ˆä»…åœ¨plainæ ¼å¼æ—¶æ˜¾ç¤ºï¼‰
        if format == "plain":
            click.echo("ğŸ” å¼€å§‹æ‰«æé‡å¤æ–‡ä»¶...")

        # æ‰§è¡Œæ£€æµ‹
        duplicate_groups, all_files = _execute_duplicate_finder(finder)

        # æ‰§è¡ŒAIåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        ai_analyses = None
        if ai_analyze and duplicate_groups:
            # è·å–AIé…ç½®
            ai_config = AIConfig()
            if not ai_config.enabled:
                click.echo("\nâš ï¸  AIåŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
            else:
                click.echo("\nğŸ¤– æ­£åœ¨è¿›è¡ŒAIç‰ˆæœ¬åˆ†æ...")
                # è¿è¡Œå¼‚æ­¥AIåˆ†æ
                ai_analyses = asyncio.run(_perform_ai_analysis(duplicate_groups, ai_config))
                click.echo("âœ… AIåˆ†æå®Œæˆ\n")


        # æ ¹æ®æ ¼å¼é€‰æ‹©è¾“å‡ºæ–¹å¼
        if format != "plain":
            # å¦‚æœæœ‰AIåˆ†æï¼Œæ·»åŠ åˆ°JSON/CSVè¾“å‡º
            if ai_analyses:
                # å¢å¼ºæ ¼å¼åŒ–æ•°æ®
                groups_data = []
                for group in duplicate_groups:
                    group_data = {
                        "hash": group.hash,
                        "size": group.size,
                        "count": group.count,
                        "files": [str(f) for f in group.files],
                    }
                    # æ·»åŠ AIåˆ†æç»“æœ
                    if group.hash in ai_analyses:
                        data_key = f"{group.hash}_data"
                        if data_key in ai_analyses:
                            group_data["ai_recommendation"] = str(ai_analyses[data_key].get("recommended_keep", ""))
                            group_data["ai_confidence"] = ai_analyses[data_key].get("confidence", 0.0)
                    groups_data.append(group_data)

                # åˆ›å»ºå¢å¼ºçš„æ•°æ®æ¨¡å‹
                total_save_space = sum(group.potential_save for group in duplicate_groups)
                data = DuplicateData(
                    total_groups=len(duplicate_groups),
                    total_size_saved=total_save_space,
                    groups=groups_data,
                )
                output = format_output(data, format)
                click.echo(output)
            else:
                _handle_formatted_output(duplicate_groups, format)
        else:
            # ä¿æŒåŸæœ‰çš„çº¯æ–‡æœ¬è¾“å‡ºæ–¹å¼
            display_duplicate_results(
                duplicate_groups=duplicate_groups,
                scan_path=os.path.abspath(path),
                total_files=len(all_files),
                recursive=duplicate_config.recursive,
                show_commands=show_commands,
            )

        # è®°å½•åˆ°æ“ä½œå†å²
        from ..utils.smart_interactive import operation_history

        # è®¡ç®—æ€»çš„èŠ‚çœç©ºé—´
        total_save_space = sum(group.potential_save for group in duplicate_groups)

        operation_history.add(
            "duplicates",
            {
                "path": path,
                "recursive": duplicate_config.recursive,
                "min_size": duplicate_config.min_size,
                "extensions": duplicate_config.extensions,
                "format": format,
                "ai_analyze": ai_analyze,  # è®°å½•æ˜¯å¦ä½¿ç”¨AI
            },
            {
                "scanned_files": len(all_files),
                "duplicate_groups": len(duplicate_groups),
                "space_saved": total_save_space,
            },
        )

    except ToolError:
        # ToolError ä¼šè¢« handle_errors è£…é¥°å™¨è‡ªåŠ¨å¤„ç†
        raise
    except click.ClickException:
        # Clickå¼‚å¸¸ç›´æ¥ä¼ æ’­
        raise
    except Exception as e:
        # å…¶ä»–å¼‚å¸¸è½¬æ¢ä¸ºToolError
        logfire.error(f"é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥: {str(e)}")
        context = ErrorContext(
            operation="é‡å¤æ–‡ä»¶æ£€æµ‹",
            details={"error_type": "æœªçŸ¥é”™è¯¯", "original_error": str(e)},
        )
        raise ToolError(
            "é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥",
            error_code="GENERAL_ERROR",
            context=context,
            original_error=e,
            suggestions=[
                "æ£€æŸ¥è¾“å…¥å‚æ•°æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤ç›®å½•æƒé™è®¾ç½®",
                "å°è¯•é‡æ–°è¿è¡Œå‘½ä»¤",
                "å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æŠ¥å‘Šæ­¤é”™è¯¯",
            ],
        )
