"""é‡å¤æ–‡ä»¶æ£€æµ‹å·¥å…· - æ™ºèƒ½æŸ¥æ‰¾ç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶."""

import hashlib
import os
from collections import defaultdict
from pathlib import Path
from typing import Optional

import click
import logfire
from pydantic import BaseModel, Field

from simple_tools._typing import argument, command, option, pass_context

from ..utils.formatter import DuplicateData, OutputFormat, format_output
from ..utils.progress import process_with_progress


class DuplicateConfig(BaseModel):
    """é‡å¤æ–‡ä»¶æ£€æµ‹é…ç½®."""

    path: str = Field(..., description="æ‰«æè·¯å¾„")
    recursive: bool = Field(True, description="æ˜¯å¦é€’å½’æ‰«æå­ç›®å½•")
    min_size: int = Field(1, description="æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    extensions: Optional[list[str]] = Field(None, description="æŒ‡å®šæ–‡ä»¶æ‰©å±•å")


class FileInfo(BaseModel):
    """æ–‡ä»¶ä¿¡æ¯."""

    path: Path = Field(..., description="æ–‡ä»¶è·¯å¾„")
    size: int = Field(..., description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    hash: Optional[str] = Field(None, description="æ–‡ä»¶MD5å“ˆå¸Œå€¼")


class DuplicateGroup(BaseModel):
    """é‡å¤æ–‡ä»¶ç»„."""

    hash: str = Field(..., description="æ–‡ä»¶MD5å“ˆå¸Œå€¼")
    size: int = Field(..., description="æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰")
    count: int = Field(..., description="é‡å¤æ–‡ä»¶æ•°é‡")
    files: list[Path] = Field(..., description="é‡å¤æ–‡ä»¶è·¯å¾„åˆ—è¡¨")
    potential_save: int = Field(..., description="å¯èŠ‚çœç©ºé—´ï¼ˆå­—èŠ‚ï¼‰ä¿ç•™ä¸€ä¸ªæ–‡ä»¶")


class DuplicateFinder:
    """é‡å¤æ–‡ä»¶æ£€æµ‹å™¨."""

    def __init__(self, config: DuplicateConfig):
        """åˆå§‹åŒ–é‡å¤æ–‡ä»¶æ£€æµ‹å™¨.

        å‚æ•°ï¼šconfig - æ£€æµ‹é…ç½®å¯¹è±¡
        """
        self.config = config
        logfire.info("åˆå§‹åŒ–é‡å¤æ–‡ä»¶æ£€æµ‹å™¨", attributes={"config": config.model_dump()})

    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼ï¼ˆåˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜é—®é¢˜ï¼‰.

        å‚æ•°ï¼šfile_path - æ–‡ä»¶è·¯å¾„å¯¹è±¡
        è¿”å›ï¼šMD5å“ˆå¸Œå€¼å­—ç¬¦ä¸²
        """
        hash_md5 = hashlib.md5()

        try:
            with open(file_path, "rb") as f:
                # æ¯æ¬¡è¯»å–8KBï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
                for chunk in iter(lambda: f.read(8192), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logfire.error(
                f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {file_path}", attributes={"error": str(e)}
            )
            raise

    def _should_include_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥åŒ…å«åœ¨æ£€æµ‹èŒƒå›´å†….

        å‚æ•°ï¼šfile_path - æ–‡ä»¶è·¯å¾„å¯¹è±¡
        è¿”å›ï¼šå¸ƒå°”å€¼ï¼ŒTrueè¡¨ç¤ºåº”è¯¥åŒ…å«
        """
        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
        try:
            file_size = file_path.stat().st_size
            if file_size < self.config.min_size:
                return False
        except OSError:
            return False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦åœ¨æŒ‡å®šåˆ—è¡¨ä¸­
        if self.config.extensions:
            file_ext = file_path.suffix.lower()
            return file_ext in [ext.lower() for ext in self.config.extensions]

        return True

    def _scan_files(self) -> list[FileInfo]:
        """æ‰«æç›®å½•è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ä¿¡æ¯.

        è¿”å›ï¼šFileInfoå¯¹è±¡åˆ—è¡¨
        """
        with logfire.span("scan_files", attributes={"path": self.config.path}):
            files = []
            scan_path = Path(self.config.path)

            try:
                # æ ¹æ®é…ç½®é€‰æ‹©æ‰«ææ–¹å¼
                if self.config.recursive:
                    pattern = "**/*"
                else:
                    pattern = "*"

                # æ‰«ææ–‡ä»¶
                for file_path in scan_path.glob(pattern):
                    # åªå¤„ç†æ–‡ä»¶ï¼Œè·³è¿‡ç›®å½•
                    if not file_path.is_file():
                        continue

                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«æ­¤æ–‡ä»¶
                    if not self._should_include_file(file_path):
                        continue

                    # è·å–æ–‡ä»¶å¤§å°
                    file_size = file_path.stat().st_size

                    # åˆ›å»ºæ–‡ä»¶ä¿¡æ¯å¯¹è±¡
                    files.append(FileInfo(path=file_path, size=file_size))

                logfire.info(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
                return files

            except Exception as e:
                logfire.error(f"æ‰«ææ–‡ä»¶å¤±è´¥: {str(e)}")
                raise

    def find_duplicates(self) -> list[DuplicateGroup]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶.

        è¿”å›ï¼šDuplicateGroupå¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªç»„åŒ…å«é‡å¤çš„æ–‡ä»¶
        """
        with logfire.span("find_duplicates"):
            logfire.info("å¼€å§‹é‡å¤æ–‡ä»¶æ£€æµ‹")

            # ç¬¬ä¸€æ­¥ï¼šæ‰«ææ‰€æœ‰æ–‡ä»¶
            all_files = self._scan_files()
            if not all_files:
                logfire.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
                return []

            # ç¬¬äºŒæ­¥ï¼šæŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„
            size_groups = self._group_files_by_size(all_files)
            # è¿‡æ»¤æ‰åªæœ‰ä¸€ä¸ªæ–‡ä»¶çš„å¤§å°ç»„
            potential_duplicates = {
                size: files for size, files in size_groups.items() if len(files) > 1
            }
            logfire.info(
                f"æŒ‰å¤§å°åˆ†ç»„åï¼Œ{len(potential_duplicates)} ä¸ªå¤§å°ç»„å¯èƒ½åŒ…å«é‡å¤æ–‡ä»¶"
            )

            # ç¬¬ä¸‰æ­¥ï¼šç»„è£…å“ˆå¸Œä»»åŠ¡
            all_files_to_hash = self._collect_files_to_hash(potential_duplicates)
            logfire.info(f"éœ€è¦è®¡ç®— {len(all_files_to_hash)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼")

            # ç¬¬å››æ­¥ï¼šæ‰¹é‡è®¡ç®—å“ˆå¸Œå¹¶åˆ†ç»„
            size_hash_groups = self._group_files_by_hash(all_files_to_hash)

            # ç¬¬äº”æ­¥ï¼šç»„è£…æœ€ç»ˆé‡å¤ç»„
            duplicate_groups = self._assemble_duplicate_groups(size_hash_groups)

            # æŒ‰å¯èŠ‚çœç©ºé—´æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰
            duplicate_groups.sort(key=lambda x: x.potential_save, reverse=True)
            logfire.info(f"æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(duplicate_groups)} ç»„é‡å¤æ–‡ä»¶")
            return duplicate_groups

    def _group_files_by_size(
        self, all_files: list["FileInfo"]
    ) -> dict[int, list["FileInfo"]]:
        """æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„."""
        groups = defaultdict(list)
        for file_info in all_files:
            groups[file_info.size].append(file_info)
        return groups

    def _collect_files_to_hash(
        self, potential_duplicates: dict[int, list["FileInfo"]]
    ) -> list[tuple[int, "FileInfo"]]:
        """ç»„è£…éœ€è¦è®¡ç®—å“ˆå¸Œçš„æ–‡ä»¶åˆ—è¡¨."""
        all_files_to_hash = []
        for file_size, files in potential_duplicates.items():
            all_files_to_hash.extend([(file_size, file_info) for file_info in files])
        return all_files_to_hash

    def _group_files_by_hash(
        self, all_files_to_hash: list[tuple[int, FileInfo]]
    ) -> dict[int, dict[str, list[FileInfo]]]:
        """æ‰¹é‡è®¡ç®—å“ˆå¸Œå¹¶æŒ‰å¤§å°å’Œå“ˆå¸Œåˆ†ç»„."""

        def calculate_hash_for_file(
            file_data: tuple[int, FileInfo],
        ) -> Optional[tuple[int, FileInfo, str]]:
            file_size, file_info = file_data
            try:
                file_hash = self._calculate_file_hash(file_info.path)
                file_info.hash = file_hash
                return (file_size, file_info, file_hash)
            except Exception as e:
                logfire.warning(f"è·³è¿‡æ–‡ä»¶ {file_info.path}: {str(e)}")
                return None

        size_hash_groups: dict[int, dict[str, list[FileInfo]]] = defaultdict(
            lambda: defaultdict(list)
        )
        results = process_with_progress(
            all_files_to_hash,
            calculate_hash_for_file,
            label="è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼",
            threshold=100,
        )
        for result in results:
            if result is not None:
                file_size, file_info, file_hash = result
                size_hash_groups[file_size][file_hash].append(file_info)
        # è½¬ä¸ºå¸¸è§„ dict è¿”å›ï¼Œä¿è¯ç±»å‹ä¸€è‡´
        return {size: dict(hash_group) for size, hash_group in size_hash_groups.items()}

    def _assemble_duplicate_groups(
        self, size_hash_groups: dict[int, dict[str, list["FileInfo"]]]
    ) -> list["DuplicateGroup"]:
        """ç»„è£…æœ€ç»ˆçš„é‡å¤æ–‡ä»¶ç»„."""
        duplicate_groups = []
        for file_size, hash_groups in size_hash_groups.items():
            for file_hash, duplicate_files in hash_groups.items():
                if len(duplicate_files) > 1:
                    potential_save = file_size * (len(duplicate_files) - 1)
                    duplicate_group = DuplicateGroup(
                        hash=file_hash,
                        size=file_size,
                        count=len(duplicate_files),
                        files=[f.path for f in duplicate_files],
                        potential_save=potential_save,
                    )
                    duplicate_groups.append(duplicate_group)
        return duplicate_groups


def format_size(size_bytes: int) -> str:
    """å°†å­—èŠ‚æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»çš„æ–‡ä»¶å¤§å°æ ¼å¼.

    å¤ç”¨file_tool.pyä¸­çš„å‡½æ•°é€»è¾‘
    """
    if size_bytes == 0:
        return "0 B"

    units = ["B", "KB", "MB", "GB"]
    unit_index = 0
    size = float(size_bytes)

    while size >= 1024 and unit_index < len(units) - 1:
        size /= 1024
        unit_index += 1

    return f"{size:.1f} {units[unit_index]}"


def display_duplicate_results(
    duplicate_groups: list[DuplicateGroup],
    scan_path: str,
    total_files: int,
    recursive: bool,
    show_commands: bool = False,
) -> None:
    """å‹å¥½å±•ç¤ºé‡å¤æ–‡ä»¶æ£€æµ‹ç»“æœ.

    å‚æ•°ï¼š
        duplicate_groups - é‡å¤æ–‡ä»¶ç»„åˆ—è¡¨
        scan_path - æ‰«æè·¯å¾„
        total_files - æ‰«æçš„æ–‡ä»¶æ€»æ•°
        recursive - æ˜¯å¦é€’å½’æ‰«æ
        show_commands - æ˜¯å¦æ˜¾ç¤ºåˆ é™¤å»ºè®®å‘½ä»¤
    """
    # è¾“å‡ºæ‰«æä¿¡æ¯
    click.echo(f"æ‰«æç›®å½•: {scan_path}")
    click.echo(f"æ‰«ææ¨¡å¼: {'é€’å½’æ‰«æ' if recursive else 'ä»…é¡¶å±‚ç›®å½•'}")
    click.echo(f"æ–‡ä»¶æ€»æ•°: {total_files:,} ä¸ª")
    click.echo("â”" * 60)

    # å¦‚æœæ²¡æœ‰é‡å¤æ–‡ä»¶
    if not duplicate_groups:
        click.echo("ğŸ‰ æœªå‘ç°é‡å¤æ–‡ä»¶ï¼")
        return

    # æ˜¾ç¤ºé‡å¤æ–‡ä»¶ç»„
    click.echo(f"å‘ç° {len(duplicate_groups)} ç»„é‡å¤æ–‡ä»¶ï¼š\n")

    total_duplicates = 0
    total_save_space = 0

    for index, group in enumerate(duplicate_groups, 1):
        total_duplicates += group.count
        total_save_space += group.potential_save

        # æ˜¾ç¤ºç»„æ ‡é¢˜
        click.echo(
            f"ã€ç¬¬ {index} ç»„ã€‘{group.count} ä¸ªæ–‡ä»¶, "
            f"æ¯ä¸ª {format_size(group.size)}, "
            f"å¯èŠ‚çœ {format_size(group.potential_save)}"
        )

        # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
        for file_path in group.files:
            click.echo(f"  â€¢ {file_path}")

        # å¦‚æœéœ€è¦æ˜¾ç¤ºåˆ é™¤å‘½ä»¤å»ºè®®
        if show_commands and len(group.files) > 1:
            click.echo(f"  ğŸ’¡ å»ºè®®ä¿ç•™: {group.files[0]}")
            click.echo("  ğŸ—‘ï¸  å¯åˆ é™¤å‘½ä»¤:")
            for file_path in group.files[1:]:
                click.echo(f"     rm '{file_path}'")

        click.echo()  # ç©ºè¡Œåˆ†éš”

    # æ˜¾ç¤ºæ€»ç»“ç»Ÿè®¡
    click.echo("â”" * 60)
    click.echo(
        f"æ€»è®¡ï¼š{total_duplicates} ä¸ªé‡å¤æ–‡ä»¶ï¼Œ"
        f"å¯èŠ‚çœ {format_size(total_save_space)} ç©ºé—´"
    )

    if show_commands:
        click.echo("\nâš ï¸  è­¦å‘Šï¼šåˆ é™¤æ–‡ä»¶å‰è¯·ç¡®è®¤é‡è¦æ€§ï¼Œå»ºè®®å…ˆå¤‡ä»½ï¼")


# ä¿®æ”¹ duplicates_cmd å‡½æ•°ï¼Œæ·»åŠ  format å‚æ•°
@command()
@argument("path", type=click.Path(exists=True), default=".")
@option(
    "-r", "--recursive", is_flag=True, default=True, help="é€’å½’æ‰«æå­ç›®å½•ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
)
@option("-n", "--no-recursive", is_flag=True, help="ä»…æ‰«æé¡¶å±‚ç›®å½•ï¼Œä¸é€’å½’")
@option("-s", "--min-size", type=int, default=1, help="æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤1")
@option(
    "-e",
    "--extension",
    multiple=True,
    help="æŒ‡å®šæ–‡ä»¶æ‰©å±•åï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼‰ï¼Œå¦‚ï¼š-e .jpg -e .png",
)
@option("--show-commands", is_flag=True, help="æ˜¾ç¤ºåˆ é™¤é‡å¤æ–‡ä»¶çš„å»ºè®®å‘½ä»¤")
@option(
    "--format",
    type=click.Choice(["plain", "json", "csv"], case_sensitive=False),
    default="plain",
    help="è¾“å‡ºæ ¼å¼ï¼ˆplain/json/csvï¼‰",
)
@pass_context
def duplicates_cmd(
    ctx: click.Context,
    path: str,
    recursive: bool,
    no_recursive: bool,
    min_size: int,
    extension: tuple[str, ...],
    show_commands: bool,
    format: str,
) -> None:
    """æŸ¥æ‰¾æŒ‡å®šç›®å½•ä¸­çš„é‡å¤æ–‡ä»¶.

    PATH: è¦æ‰«æçš„ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰

    ç¤ºä¾‹ç”¨æ³•ï¼š
      tools duplicates .                    # æ‰«æå½“å‰ç›®å½•
      tools duplicates ~/Downloads -n       # åªæ‰«æDownloadsé¡¶å±‚
      tools duplicates . -s 1048576        # åªæŸ¥æ‰¾å¤§äº1MBçš„æ–‡ä»¶
      tools duplicates . -e .jpg -e .png   # åªæŸ¥æ‰¾å›¾ç‰‡æ–‡ä»¶
      tools duplicates . --show-commands   # æ˜¾ç¤ºåˆ é™¤å»ºè®®
      tools duplicates . --format json     # JSONæ ¼å¼è¾“å‡º
    """
    # å¤„ç†é€’å½’é€‰é¡¹å†²çª
    if no_recursive:
        recursive = False

    # è½¬æ¢æ‰©å±•ååˆ—è¡¨
    extensions = list(extension) if extension else None

    try:
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = DuplicateConfig(
            path=path, recursive=recursive, min_size=min_size, extensions=extensions
        )

        # åˆ›å»ºæ£€æµ‹å™¨å¹¶æ‰§è¡Œæ£€æµ‹
        finder = DuplicateFinder(config)

        # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯ï¼ˆä»…åœ¨plainæ ¼å¼æ—¶æ˜¾ç¤ºï¼‰
        if format == "plain":
            click.echo("ğŸ” å¼€å§‹æ‰«æé‡å¤æ–‡ä»¶...")

        # æ‰§è¡Œæ£€æµ‹
        duplicate_groups = finder.find_duplicates()

        # è®¡ç®—æ‰«æçš„æ€»æ–‡ä»¶æ•°ï¼ˆç”¨äºç»Ÿè®¡æ˜¾ç¤ºï¼‰
        all_files = finder._scan_files()
        total_files = len(all_files)

        # æ ¹æ®æ ¼å¼é€‰æ‹©è¾“å‡ºæ–¹å¼
        if format != "plain":
            # è®¡ç®—æ€»çš„èŠ‚çœç©ºé—´
            total_save_space = sum(group.potential_save for group in duplicate_groups)

            # æ„å»ºæ ¼å¼åŒ–æ•°æ®
            groups_data = []
            for group in duplicate_groups:
                groups_data.append(
                    {
                        "hash": group.hash,
                        "size": group.size,
                        "count": group.count,
                        "files": [str(f) for f in group.files],
                    }
                )

            # åˆ›å»ºæ•°æ®æ¨¡å‹
            data = DuplicateData(
                total_groups=len(duplicate_groups),
                total_size_saved=total_save_space,
                groups=groups_data,
            )

            # æ ¼å¼åŒ–è¾“å‡º
            output = format_output(data, OutputFormat(format))
            click.echo(output)
        else:
            # ä¿æŒåŸæœ‰çš„çº¯æ–‡æœ¬è¾“å‡ºæ–¹å¼
            display_duplicate_results(
                duplicate_groups=duplicate_groups,
                scan_path=os.path.abspath(path),
                total_files=total_files,
                recursive=recursive,
                show_commands=show_commands,
            )

    except click.ClickException:
        # Clickå¼‚å¸¸ç›´æ¥ä¼ æ’­
        raise
    except Exception as e:
        # å…¶ä»–å¼‚å¸¸è½¬æ¢ä¸ºClickå¼‚å¸¸
        logfire.error(f"é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥: {str(e)}")
        raise click.ClickException(f"é”™è¯¯ï¼š{str(e)}")
